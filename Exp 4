from tensorflow.keras import models
from tensorflow.keras.layers import Dense, Dropout

def mlp_model(layers, units, dropout_rate, input_shape, num_classes):
    """Creates an instance of a multi-layer perceptron model.

    # Arguments
        layers: int, number of 'Dense' layers in the model.
        units: int, output dimension of the layers.
        dropout_rate: float, percentage of input to drop at Dropout layers.
        input_shape: tuple, shape of input to the model.
        num_classes: int, number of output classes.

    # Returns
        An MLP model instance.
    """
    def _get_last_layer_units_and_activation(num_classes):
        # Determine the number of units and activation function for the output layer
        if num_classes == 1:
            return 1, 'sigmoid'  # for binary classification
        else:
            return num_classes, 'softmax'  # for multi-class classification

    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)

    model = models.Sequential()
    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))  # Dropout layer with input shape

    # Add the specified number of Dense layers with ReLU activation
    for _ in range(layers - 1):
        model.add(Dense(units=units, activation='relu'))  # Add hidden layers
        model.add(Dropout(rate=dropout_rate))  # Add Dropout after hidden layers

    # Add the final output layer
    model.add(Dense(units=op_units, activation=op_activation))

    return model

# Example usage for a binary classification problem
model = mlp_model(layers=3, units=64, dropout_rate=0.5, input_shape=(10,), num_classes=1)
model.summary()
